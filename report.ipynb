{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Political Analysis\n",
    "## Insight to behavior of Politically Active Redditors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Reddit as a Social Network\n",
    "\n",
    "This report deals heavily with the nature of reddit and how users interact with the platform. As such, it is imperative that first these characteristics are outlined, and fully understood, as they form the presmise of our analysis and later discussion.\n",
    "\n",
    "Reddit is a social media network centered around forum-based interactions. Lying somewhere in between 4Chan and Facebook, Reddit shifts focus away from the user, focusing instead on content-specific communities, while not going so far as to be completely anonymous. These communitites tend to be cliquey, in that they tend to have specific vernacular in the form of inside jokes and jargon: for instance, HighQualityGifs tends to have meta-discussion on GIF-making. However, users are not restricted to a single community and can participate in all of them. This leads to the the two most important, characteristics of the Reddit: \n",
    "    \n",
    "1. Reddit is essentially comprised of Subreddits\n",
    "2. Users interact on Subreddits, and are free to do so on any subreddit.\n",
    "    \n",
    "### Political Activism on Reddit\n",
    "\n",
    "Reddit is highly politically active. The platform lends itself to confrontation and conversation between people of varying backgrounds and political leanings, more-so than Facebook, and even Twitter, as people aren't restricted by their own friend circles. Differing political factions pour out of their respective subreddits into common spaces across Reddit, influencing the nature of discussion in the mainstream. More-so, the platform is an important source of news to manny of its users. According to a study by the Pew Research Center, though only less than 1 in 10 American Adults use Reddit, more than 7 in 10 of users rely on it as their primary news source. This means that Reddit is an important place to look to see how people consume news in the Age of the Internet. \n",
    "\n",
    "The epicenter of political activism on the platform lies in . There are subreddits for most every type of political affiliation, though some more extreme ideologies have had their subreddits shut down i.e r/nationalsocialism. The most famous of these subreddits is r/The_Donald, with 600,000 subs. The_Donald is notorious across the platform for being highly insular, really only having content that conforms to what the community wants to hear. This is true for most if all of the subreddits on both the left and right. \n",
    "\n",
    "### The Problem: \n",
    "\n",
    "All this being said, what we wanted to look at was just how insular Reddit is, evaulating whether Reddit could reasonably split into a \"Left\" and \"Right\" reddit, each running in their own echo chambers. This is a point of interest because it would let us get a better understanding of:\n",
    "\n",
    "1. Reddit's efficacy as an informative News Source\n",
    "2. Organic Organization of Political Activist groups\n",
    "3. The differing structure of \"Leftist\" Reddit and \"Right\" Reddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Collection\n",
    "\n",
    "We decided that the approrpiate way to approach the problem was to garner user data from politicized subreddits and see how the spread themselves across the general subreddits. We gathered this user data using PRAW, the Python Reddit API Wrapper to collect data from the Reddit API.  \n",
    "\n",
    "To gather training data, we created a list of left-leaning and right-leaning subreddits and gathered lists of contributors and the subreddits they contributed to. The idea was that these users could be classified as left-users and right-users respectively. By looking at the subreddits that they contribute too regularly, we could see the number of contributers for each subreddit on the right and left. Because each redditor’s subscription information is private, \n",
    "we used each user’s top 100 comments of the past month to determine which subreddits they recently contributed to. For the training data, we were able to collect: 17999 Redditors in right-leaning subreddits, 22059 Redditors in left-leaning subreddits, for a total of 659,005 total contributions to unique subreddits.\n",
    "\n",
    "For test data, we collected  30771 Redditors who participated in discussion on the top posts of the month across all of Reddit (which to be sure does not include any posts from the subreddits in the test data). The idea here being that we would be able to classify these users using an classifier trained on the training data, to get a better understanding of how this relates to mainstream reddit. The test data ended up being comprised of 538602 total contributions to unique subreddits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import praw, json, sys\n",
    "from pprint import pprint\n",
    "\n",
    "reddit = praw.Reddit(client_id=sys.argv[1],\n",
    "                     client_secret=sys.argv[2],\n",
    "                     user_agent='Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \\\n",
    "                     (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36')\n",
    "sub = sys.argv[3].lower()\n",
    "print(sub)\n",
    "\n",
    "users = set()\n",
    "usernames = set()\n",
    "user_subs = dict()\n",
    "\n",
    "with open('users/' + sub + '.json', 'w') as f:\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    for post in subreddit.top('month'): # search 100 top posts of the month\n",
    "        post.comments.replace_more(limit=None) # all comments on each post\n",
    "        comments = post.comments.list()\n",
    "        for comment in comments:\n",
    "            if comment.author != None:\n",
    "                users.add(comment.author)\n",
    "                usernames.add(comment.author.name)\n",
    "                subs = user_subs.get(comment.author.name, set())\n",
    "                subs.add(sub)\n",
    "                user_subs[comment.author.name] = subs\n",
    "\n",
    "    json.dump(list(usernames), f, separators=(',', ':'))\n",
    "\n",
    "print(len(usernames))\n",
    "with open('subs/' + sub + '.json', 'w') as f:\n",
    "    for user in list(users):\n",
    "        try:\n",
    "            for comment in user.comments.top('month'): # search 100 top comments\n",
    "                subs = user_subs[user.name]\n",
    "                subs.add(str(comment.subreddit).lower())\n",
    "                user_subs[user.name] = subs\n",
    "        except:\n",
    "            pass\n",
    "    for user in user_subs:\n",
    "        user_subs[user] = list(user_subs[user])\n",
    "    json.dump(user_subs, f, separators=(',', ':'))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('reddit data/all_subs.json') as subs_json:\n",
    "    subreddits = json.load(subs_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18069\n",
      "['moviepass', 'greendawn', 'disneyporn', 'lithuaniaspheres', 'meghanmarkle', 'anarchy101', 'linuxfromscratch', 'lojban', 'hvacadvice', 'ratemysinging']\n"
     ]
    }
   ],
   "source": [
    "print(len(subreddits))\n",
    "print(subreddits[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('reddit data/left.json') as left_json:\n",
    "    left = json.load(left_json)\n",
    "with open('reddit data/right.json') as right_json:\n",
    "    right = json.load(right_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left User Count: 22059, Right User Count: 17999\n"
     ]
    }
   ],
   "source": [
    "print(\"Left User Count: %d, Right User Count: %d\" % (len(left), len(right)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = []\n",
    "all_users = []\n",
    "\n",
    "for user, subs in left.items():\n",
    "    all_data.append(subs)\n",
    "    all_users.append(user)\n",
    "    \n",
    "\n",
    "for user, subs in right.items():\n",
    "    all_data.append(subs)\n",
    "    all_users.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "converted_data = mlb.fit_transform(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_left = len(left)\n",
    "len_right = len(right)\n",
    "\n",
    "labels = np.append(np.zeros(len_left), np.ones(len_right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'alpha': 20.897959183673468} with a score of 0.89\n"
     ]
    }
   ],
   "source": [
    "alpha_range = np.linspace(1, 40)\n",
    "param_grid = dict(alpha=alpha_range)\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def run_grid(param_grid, classifier):\n",
    "    grid = GridSearchCV(classifier, param_grid=param_grid, cv=cv, verbose=0, n_jobs=-1)\n",
    "    grid.fit(converted_data, labels)\n",
    "    return (grid.best_params_, grid.best_score_)\n",
    "    \n",
    "best_param_nb, best_score_nb = run_grid(param_grid, BernoulliNB())\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (best_param_nb, best_score_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'alpha': 20.820512820512821} with a score of 0.89\n"
     ]
    }
   ],
   "source": [
    "alpha_range = np.linspace(20, 22, 40)\n",
    "param_grid = dict(alpha=alpha_range)\n",
    "\n",
    "best_param_nb, best_score_nb = run_grid(param_grid, BernoulliNB())\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (best_param_nb, best_score_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C_range = np.logspace(1, 3, 4)\n",
    "gamma_range = np.logspace(-6, -2, 5)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "\n",
    "best_param_svm, best_score_svm = run_grid(param_grid, svm.SVC())\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (best_param_svm, best_score_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C_range = np.logspace(1.9, 2.1, 4)\n",
    "gamma_range = np.logspace(-4.1, -3.9, 4)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "\n",
    "best_param_svm, best_score_svm = run_grid(param_grid, svm.SVC())\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (best_param_svm, best_score_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf = BernoulliNB(alpha=best_param_nb['alpha'])\n",
    "\n",
    "nb_clf.fit(converted_data, labels)\n",
    "nb_clf.score(converted_data, labels)\n",
    "\n",
    "clf = svm.SVC(C=best['C'], gamma=best['gamma'])\n",
    "svm_clf.fit(converted_data, labels)\n",
    "svm_clf.score(converted_data, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(nb_clf, 'nb.pkl') \n",
    "joblib.dump(clf, 'svm.pkl')\n",
    "joblib.dump(mlb, 'mlb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "nb_clf = joblib.load('nb.pkl')\n",
    "svm_clf = joblib.load('svm.pkl')\n",
    "mlb = joblib.load('mlb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('reddit data/test_subs.json') as test_json:\n",
    "    test = json.load(test_json)\n",
    "    \n",
    "test_data = []\n",
    "test_users = []\n",
    "\n",
    "for user, subs in test.items():\n",
    "    test_data.append(subs)\n",
    "    test_users.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_data = list(map(lambda l: list(filter(lambda sub: sub in subreddits, l)), test_data))\n",
    "\n",
    "converted_test = mlb.transform(filtered_data)\n",
    "\n",
    "predicted_labels = nb_clf.predict(converted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('reddit data/test_preds_nb.json', 'w') as test_preds:\n",
    "    json.dump(dict(zip(test_users, predicted_labels)), test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, math\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('reddit data/left.json') as f:\n",
    "    left = json.load(f)\n",
    "with open('reddit data/right.json') as f:\n",
    "    right = json.load(f)\n",
    "with open('reddit data/left_subs.json') as f:\n",
    "    left_subs = set(json.load(f))\n",
    "with open('reddit data/right_subs.json') as f:\n",
    "    right_subs = set(json.load(f))\n",
    "with open('reddit data/left_users.json') as f:\n",
    "    left_users = set(json.load(f))\n",
    "with open('reddit data/right_users.json') as f:\n",
    "    right_users = set(json.load(f))\n",
    "with open('reddit data/left_sources.txt') as f:\n",
    "    left_sources = set([l.strip().lower() for l in f.readlines() if l.strip() != ''])\n",
    "with open('reddit data/right_sources.txt') as f:\n",
    "    right_sources = set([l.strip().lower() for l in f.readlines() if l.strip() != ''])\n",
    "with open('reddit data/all_subs.json') as f:\n",
    "    all_subs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# invert user:subs dict\n",
    "sub_users = dict()\n",
    "for s in all_subs:\n",
    "    sub_users[s] = set()\n",
    "for u in left:\n",
    "    for s in left[u]:\n",
    "        sub_users[s].add(u)\n",
    "for u in right:\n",
    "    for s in right[u]:\n",
    "        sub_users[s].add(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out small subs\n",
    "counts = {s:len(sub_users[s]) for s in sub_users}\n",
    "large_subs = {s:sub_users[s] for s in sub_users if len(sub_users[s]) >= 1500}\n",
    "print(len(large_subs))\n",
    "large_sub_users = set()\n",
    "for s in large_subs:\n",
    "    for u in large_subs[s]:\n",
    "        large_sub_users.add(u)\n",
    "print(len(large_sub_users))\n",
    "\n",
    "most_subs = {s:sub_users[s] for s in sub_users if len(sub_users[s]) >= 50}\n",
    "print(len(most_subs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_file, output_notebook, reset_output\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import Plot, Range1d, MultiLine, Circle, HoverTool, BoxZoomTool, WheelZoomTool, PanTool, TapTool\n",
    "from bokeh.models import GraphRenderer, StaticLayoutProvider, Oval, Span\n",
    "from bokeh.palettes import Spectral4\n",
    "from bokeh.models.graphs import from_networkx, NodesAndLinkedEdges, EdgesAndLinkedNodes\n",
    "from bokeh.layouts import row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sub(sub):\n",
    "    users = sub_users[sub]\n",
    "    score = 0\n",
    "    for u in users:\n",
    "        if u in left_users: score -= 1\n",
    "        if u in right_users: score += 1\n",
    "    sub_score = score / len(users)\n",
    "    sub_score -= -(len(left_users) - len(right_users)) / (len(left_users) + len(right_users))\n",
    "    if sub_score > 1: sub_score = 1\n",
    "    elif sub_score < -1: sub_score = -1\n",
    "    return sub_score\n",
    "\n",
    "scored_subs = {s: score_sub(s) for s in most_subs}\n",
    "sorted_scored_subs = list(reversed(sorted(scored_subs)))\n",
    "scores = [scored_subs[s] for s in sorted_scored_subs]\n",
    "\n",
    "X_RANGE = 50\n",
    "scatter = figure(x_range=(-X_RANGE, X_RANGE), y_range=sorted_scored_subs)\n",
    "zero = Span(location=0, dimension='height', line_color='black',\n",
    "            line_width=3)\n",
    "scatter.add_layout(zero)\n",
    "scatter.circle(scores, sorted_scored_subs, size=5, fill_color=\"green\", line_color=\"black\", line_width=3)\n",
    "show(scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "political_sub_edges = dict()\n",
    "sources_set = set(left_sources).union(set(right_sources))\n",
    "sorted_subs = sorted(list(large_subs))\n",
    "\n",
    "for s in sorted(sources_set):\n",
    "    if s not in large_subs: continue\n",
    "    for u in large_subs[s]:\n",
    "        for s2 in sorted_subs:\n",
    "            if s != s2 and u in large_subs[s2]:\n",
    "                count = political_sub_edges.get((s, s2), 0)\n",
    "                political_sub_edges[(s, s2)] = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SIZE = 50\n",
    "MAX_WEIGHT = 30\n",
    "\n",
    "max_weight = max(political_sub_edges[e] for e in political_sub_edges)\n",
    "G = nx.Graph()\n",
    "for e in political_sub_edges:\n",
    "    G.add_edge(e[0], e[1], weight=political_sub_edges[e] / max_weight * MAX_WEIGHT)\n",
    "    \n",
    "def r2bgradient(score):\n",
    "    r = math.floor(255 * score)\n",
    "    b = math.floor(255 * (1 - score))\n",
    "    color = '#' + '%02x' % r + '00' + '%02x' % b\n",
    "    return color\n",
    "\n",
    "def calculate_color(s):\n",
    "    sub_score = score_sub(s) + 1\n",
    "    sub_score /= 2\n",
    "    if sub_score > 1: sub_score = 1\n",
    "    elif sub_score < 0: sub_score = 0\n",
    "    return r2bgradient(sub_score)\n",
    "    \n",
    "colors = {s: calculate_color(s) for s in G.nodes}\n",
    "left_sources_set = set(left_sources)\n",
    "right_sources_set = set(right_sources)\n",
    "    \n",
    "max_size = max(len(large_subs[s]) for s in G.nodes)\n",
    "node_sizes = {s:len(large_subs[s]) / max_size * MAX_SIZE for s in G.nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SPACING = 1000\n",
    "def x(sub):\n",
    "    return score_sub(sub) * X_SPACING\n",
    "    \n",
    "left = 0\n",
    "center = 0\n",
    "right = 0\n",
    "def y(sub):\n",
    "    global left, center, right\n",
    "    if sub in left_sources_set:\n",
    "        result = left\n",
    "        if left >= 0: left += 2\n",
    "        left *= -1\n",
    "    elif sub in right_sources_set:\n",
    "        result = right\n",
    "        if right >= 0: right += 2\n",
    "        right *= -1\n",
    "    else:\n",
    "        result = center\n",
    "        if center >= 0: center += 1\n",
    "        center *= -1\n",
    "    return result\n",
    "\n",
    "FIGURE_SIZE = 1500\n",
    "plot = figure(x_range=(-FIGURE_SIZE, FIGURE_SIZE), y_range=(-FIGURE_SIZE, FIGURE_SIZE),\n",
    "              tools='')\n",
    "\n",
    "graph = from_networkx(G, nx.spring_layout, scale=2, center=(0,0))\n",
    "\n",
    "\n",
    "hover = HoverTool(tooltips=[(\"sub name\", \"@name\"), (\"score\", \"@score\")])\n",
    "hover.show_arrow = False\n",
    "plot.add_tools(hover, BoxZoomTool(), PanTool(), WheelZoomTool(), TapTool())\n",
    "graph.node_renderer.data_source.data['name'] = list(G.nodes)\n",
    "graph.node_renderer.data_source.data['score'] = [score_sub(s) for s in G.nodes]\n",
    "\n",
    "graph_layout = {node: (x(node), y(node) * MAX_SIZE) for node in G.nodes}\n",
    "graph.layout_provider = StaticLayoutProvider(graph_layout=graph_layout)\n",
    "\n",
    "graph.edge_renderer.data_source.data[\"line_width\"] = [G.get_edge_data(a,b)['weight'] for a, b in G.edges()]\n",
    "graph.node_renderer.data_source.data['node_color'] = [colors[n] for n in G.nodes]\n",
    "graph.node_renderer.data_source.data['node_size'] = [node_sizes[n] for n in G.nodes]\n",
    "graph.node_renderer.glyph = Circle(size='node_size', fill_color={'field': 'node_color'})\n",
    "graph.edge_renderer.glyph.line_width = {'field': 'line_width'}\n",
    "\n",
    "graph.edge_renderer.glyph = MultiLine(line_color=\"#CCCCCC\", line_alpha=0.8, line_width={'field': 'line_width'})\n",
    "graph.edge_renderer.selection_glyph = MultiLine(line_color='#000000', line_width={'field': 'line_width'})\n",
    "graph.selection_policy = NodesAndLinkedEdges()\n",
    "\n",
    "plot.renderers.append(graph)\n",
    "reset_output()\n",
    "output_notebook()\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_edges(edges, large_subs, large_sub_users, sorted_subs):\n",
    "    for i in range(len(sorted_subs)):\n",
    "        s = sorted_subs[i]\n",
    "        if s not in large_subs: continue\n",
    "        for u in large_sub_users:\n",
    "            if u in large_subs[s]:\n",
    "                for j in range(i + 1, len(sorted_subs)):\n",
    "                    s2 = sorted_subs[j]\n",
    "                    if u in large_subs[s2]:\n",
    "                        count = edges.get((s, s2), 0)\n",
    "                        edges[(s, s2)] = count + 1\n",
    "\n",
    "all_sub_edges = dict()\n",
    "make_edges(all_sub_edges, large_subs, large_sub_users, sorted_subs = sorted(list(large_subs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SIZE = 50\n",
    "MAX_WEIGHT = 30\n",
    "\n",
    "max_weight = max(all_sub_edges[e] for e in all_sub_edges)\n",
    "all_G = nx.Graph()\n",
    "for e in all_sub_edges:\n",
    "    all_G.add_edge(e[0], e[1], weight=all_sub_edges[e] / max_weight * MAX_WEIGHT)\n",
    "colors = {s: calculate_color(s) for s in all_G.nodes}\n",
    "max_size = max(len(large_subs[s]) for s in all_G.nodes)\n",
    "node_sizes = {s:len(large_subs[s]) / max_size * MAX_SIZE for s in all_G.nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network(G, colors, node_sizes, scores):\n",
    "    FIGURE_SIZE = 2.1\n",
    "    plot = figure(x_range=(-FIGURE_SIZE, FIGURE_SIZE), y_range=(-FIGURE_SIZE, FIGURE_SIZE),\n",
    "                  tools='')\n",
    "\n",
    "    graph = from_networkx(G, nx.spring_layout, scale=2, center=(0,0))\n",
    "\n",
    "\n",
    "    hover = HoverTool(tooltips=[(\"sub name\", \"@name\"), (\"score\", \"@score\")])\n",
    "    hover.show_arrow = False\n",
    "    plot.add_tools(hover, BoxZoomTool(), PanTool(), WheelZoomTool(), TapTool())\n",
    "    graph.node_renderer.data_source.data['name'] = list(G.nodes)\n",
    "    graph.node_renderer.data_source.data['score'] = scores\n",
    "\n",
    "    graph.edge_renderer.data_source.data[\"line_width\"] = [G.get_edge_data(a,b)['weight'] for a, b in G.edges()]\n",
    "    graph.node_renderer.data_source.data['node_color'] = [colors[n] for n in G.nodes]\n",
    "    graph.node_renderer.data_source.data['node_size'] = [node_sizes[n] for n in G.nodes]\n",
    "    graph.node_renderer.glyph = Circle(size='node_size', fill_color={'field': 'node_color'})\n",
    "    graph.edge_renderer.glyph.line_width = {'field': 'line_width'}\n",
    "    \n",
    "    graph.edge_renderer.glyph = MultiLine(line_color=\"#CCCCCC\", line_alpha=0.8, line_width={'field': 'line_width'})\n",
    "    graph.edge_renderer.selection_glyph = MultiLine(line_color='#000000', line_width={'field': 'line_width'})\n",
    "    graph.selection_policy = NodesAndLinkedEdges()\n",
    "\n",
    "    plot.renderers.append(graph)\n",
    "    reset_output()\n",
    "    output_notebook()\n",
    "    return plot\n",
    "\n",
    "original_plot = plot_network(all_G, colors, node_sizes, [score_sub(s) for s in all_G.nodes])\n",
    "show(original_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logs = np.rollaxis(nb_clf.feature_log_prob_, 1)\n",
    "probs = math.e ** logs\n",
    "prob_lookup = dict(zip(sorted(all_subs), probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_score(sub):\n",
    "    sub_probs = prob_lookup[sub]\n",
    "    score = sub_probs[1] / sum(sub_probs)\n",
    "    return score\n",
    "\n",
    "def prob_color(sub):\n",
    "    return r2bgradient(prob_score(sub))\n",
    "\n",
    "prob_colors = {s: prob_color(s) for s in all_G.nodes}\n",
    "prob_plot = plot_network(all_G, prob_colors, node_sizes, [prob_score(s) * 2 - 1 for s in all_G.nodes])\n",
    "show(prob_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('reddit data/test_subs.json') as f:\n",
    "    test = json.load(f)\n",
    "with open('reddit data/test_users.json') as f:\n",
    "    test_users = json.load(f)\n",
    "with open('reddit data/test_preds_nb.json') as f:\n",
    "    test_preds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert user:subs dict\n",
    "test_sub_users = dict()\n",
    "test_subs = set()\n",
    "for u in test_users:\n",
    "    test_subs.update(set(test[u]))\n",
    "for s in test_subs:\n",
    "    test_sub_users[s] = set()\n",
    "for u in test:\n",
    "    for s in test[u]:\n",
    "        test_sub_users[s].add(u)\n",
    "print(len(test_sub_users))\n",
    "test_large_subs = {s:test_sub_users[s] for s in test_sub_users if len(test_sub_users[s]) >= 1500}\n",
    "print(len(test_large_subs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sub_edges = dict()\n",
    "make_edges(test_sub_edges, test_large_subs, test_users, sorted(list(test_large_subs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SIZE = 50\n",
    "MAX_WEIGHT = 30\n",
    "\n",
    "max_weight = max(test_sub_edges[e] for e in test_sub_edges)\n",
    "test_G = nx.Graph()\n",
    "for e in test_sub_edges:\n",
    "    test_G.add_edge(e[0], e[1], weight=test_sub_edges[e] / max_weight * MAX_WEIGHT)\n",
    "    \n",
    "def predicted_score(s, users):\n",
    "    return sum(test_preds[u] for u in users) / len(users)\n",
    "\n",
    "    \n",
    "def calculate_predicted_color(s):\n",
    "    users = test_large_subs[s]\n",
    "    return r2bgradient(predicted_score(s, users))\n",
    "    \n",
    "test_colors = {s: calculate_predicted_color(s) for s in test_G.nodes}\n",
    "max_size = max(len(test_large_subs[s]) for s in test_G.nodes)\n",
    "test_node_sizes = {s:len(test_large_subs[s]) / max_size * MAX_SIZE for s in test_G.nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_plot = plot_network(test_G, test_colors, test_node_sizes,\n",
    "                              [predicted_score(s, test_sub_users[s]) * 2 - 1 for s in test_G.nodes])\n",
    "show(new_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
