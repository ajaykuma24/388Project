{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Political Analysis\n",
    "## Insight to behavior of Politically Active Redditors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Reddit as a Social Network\n",
    "\n",
    "This report deals heavily with the nature of reddit and how users interact with the platform. As such, it is imperative that first these characteristics are outlined, and fully understood, as they form the presmise of our analysis and later discussion.\n",
    "\n",
    "Reddit is a social media network centered around forum-based interactions. Lying somewhere in between 4Chan and Facebook, Reddit shifts focus away from the user, focusing instead on content-specific communities, while not going so far as to be completely anonymous. These communitites tend to be cliquey, in that they tend to have specific vernacular in the form of inside jokes and jargon: for instance, HighQualityGifs tends to have meta-discussion on GIF-making. However, users are not restricted to a single community and can participate in all of them. This leads to the the two most important, characteristics of the Reddit: \n",
    "    \n",
    "1. Reddit is essentially comprised of Subreddits\n",
    "2. Users interact on Subreddits, and are free to do so on any subreddit.\n",
    "    \n",
    "### Political Activism on Reddit\n",
    "\n",
    "Reddit is highly politically active. The platform lends itself to confrontation and conversation between people of varying backgrounds and political leanings, more-so than Facebook, and even Twitter, as people aren't restricted by their own friend circles. Differing political factions pour out of their respective subreddits into common spaces across Reddit, influencing the nature of discussion in the mainstream. More-so, the platform is an important source of news to manny of its users. According to a study by the Pew Research Center, though only less than 1 in 10 American Adults use Reddit, more than 7 in 10 of users rely on it as their primary news source. This means that Reddit is an important place to look to see how people consume news in the Age of the Internet. \n",
    "\n",
    "The epicenter of political activism on the platform lies in . There are subreddits for most every type of political affiliation, though some more extreme ideologies have had their subreddits shut down i.e r/nationalsocialism. The most famous of these subreddits is r/The_Donald, with 600,000 subs. The_Donald is notorious across the platform for being highly insular, really only having content that conforms to what the community wants to hear. This is true for most if all of the subreddits on both the left and right. \n",
    "\n",
    "### The Problem: \n",
    "\n",
    "All this being said, what we wanted to look at was just how insular Reddit is, evaulating whether Reddit could reasonably split into a \"Left\" and \"Right\" reddit, each running in their own echo chambers. This is a point of interest because it would let us get a better understanding of:\n",
    "\n",
    "1. Reddit's efficacy as an informative News Source\n",
    "2. Organic Organization of Political Activist groups\n",
    "3. The differing structure of \"Leftist\" Reddit and \"Right\" Reddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Collection\n",
    "HALp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import praw, json, sys\n",
    "from pprint import pprint\n",
    "\n",
    "reddit = praw.Reddit(client_id=sys.argv[1],\n",
    "                     client_secret=sys.argv[2],\n",
    "                     user_agent='Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \\\n",
    "                     (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36')\n",
    "sub = sys.argv[3].lower()\n",
    "print(sub)\n",
    "\n",
    "users = set()\n",
    "usernames = set()\n",
    "user_subs = dict()\n",
    "\n",
    "with open('users/' + sub + '.json', 'w') as f:\n",
    "    subreddit = reddit.subreddit(sub)\n",
    "    for post in subreddit.top('month'): # search 100 top posts of the month\n",
    "        post.comments.replace_more(limit=None) # all comments on each post\n",
    "        comments = post.comments.list()\n",
    "        for comment in comments:\n",
    "            if comment.author != None:\n",
    "                users.add(comment.author)\n",
    "                usernames.add(comment.author.name)\n",
    "                subs = user_subs.get(comment.author.name, set())\n",
    "                subs.add(sub)\n",
    "                user_subs[comment.author.name] = subs\n",
    "\n",
    "    json.dump(list(usernames), f, separators=(',', ':'))\n",
    "\n",
    "print(len(usernames))\n",
    "with open('subs/' + sub + '.json', 'w') as f:\n",
    "    for user in list(users):\n",
    "        try:\n",
    "            for comment in user.comments.top('month'): # search 100 top comments\n",
    "                subs = user_subs[user.name]\n",
    "                subs.add(str(comment.subreddit).lower())\n",
    "                user_subs[user.name] = subs\n",
    "        except:\n",
    "            pass\n",
    "    for user in user_subs:\n",
    "        user_subs[user] = list(user_subs[user])\n",
    "    json.dump(user_subs, f, separators=(',', ':'))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('reddit data/all_subs.json') as subs_json:\n",
    "    subreddits = json.load(subs_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(subreddits))\n",
    "print(subreddits[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('reddit data/left.json') as left_json:\n",
    "    left = json.load(left_json)\n",
    "with open('reddit data/right.json') as right_json:\n",
    "    right = json.load(right_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Left User Count: %d, Right User Count: %d\" % (len(left), len(right)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = []\n",
    "all_users = []\n",
    "\n",
    "for user, subs in left.items():\n",
    "    all_data.append(subs)\n",
    "    all_users.append(user)\n",
    "    \n",
    "\n",
    "for user, subs in right.items():\n",
    "    all_data.append(subs)\n",
    "    all_users.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "converted_data = mlb.fit_transform(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_left = len(left)\n",
    "len_right = len(right)\n",
    "\n",
    "labels = np.append(np.zeros(len_left), np.ones(len_right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_range = np.linspace(1, 40)\n",
    "param_grid = dict(alpha=alpha_range)\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def run_grid(param_grid, classifier):\n",
    "    grid = GridSearchCV(, param_grid=param_grid, cv=cv, verbose=0, n_jobs=-1)\n",
    "    grid.fit(converted_data, labels)\n",
    "    return (grid.best_params_, grid.best_score_)\n",
    "    \n",
    "best_param_nb, best_score_nb = run_grid(param_grid, BernoulliNB())\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (best_param, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_range = np.linspace(20, 22, 40)\n",
    "param_grid = dict(alpha=alpha_range)\n",
    "\n",
    "best_param_nb, best_score_nb = run_grid(param_grid, BernoulliNB())\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (best_param, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C_range = np.logspace(1, 3, 4)\n",
    "gamma_range = np.logspace(-6, -2, 5)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "\n",
    "best_param_svm, best_score_svm = run_grid(param_grid, svm.SVC())\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (best_param_svm, best_score_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C_range = np.logspace(1.9, 2.1, 4)\n",
    "gamma_range = np.logspace(-4.1, -3.9, 4)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "\n",
    "best_param_svm, best_score_svm = run_grid(param_grid, svm.SVC())\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (best_param_svm, best_score_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_clf = BernoulliNB(alpha=best_param_svm)\n",
    "\n",
    "nb_clf.fit(converted_data, labels)\n",
    "nb_clf.score(converted_data, labels)\n",
    "\n",
    "clf = svm.SVC(C=best['C'], gamma=best['gamma'])\n",
    "svm_clf.fit(converted_data, labels)\n",
    "svm_clf.score(converted_data, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(classifier, 'nb.pkl') \n",
    "joblib.dump(clf, 'svm.pkl')\n",
    "joblib.dump(mlb, 'mlb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "svm_clf = joblib.load('svm.pkl')\n",
    "mlb = joblib.load('mlb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('reddit data/test_subs.json') as test_json:\n",
    "    test = json.load(test_json)\n",
    "    \n",
    "test_data = []\n",
    "test_users = []\n",
    "\n",
    "for user, subs in test.items():\n",
    "    test_data.append(subs)\n",
    "    test_users.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_data = list(map(lambda l: list(filter(lambda sub: sub in subreddits, l)), test_data))\n",
    "\n",
    "converted_test = mlb.transform(filtered_data)\n",
    "\n",
    "predicted_labels = svm_clf.predict(converted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('reddit data/test_preds.json', 'w') as test_preds:\n",
    "    json.dump(dict(zip(test_users, predicted_labels)), test_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
